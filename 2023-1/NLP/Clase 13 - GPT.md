Generative Pretrained Transformers

Se elimina el encoder

# Transformer decoder

![[Pasted image 20230426154149.png]]

Mask Multi-Head Att, es causal (solo ve para atr√°s). Y se realizan en paralelo.

Se agrega una conv.

Se divide el texto completo, en oraciones mas chicas. 